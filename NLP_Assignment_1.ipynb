{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain One-Hot Encoding?**\n",
        "\n",
        "**Ans:**It is a technique used to convert categorical data into a numerical form that can be used in machine learning algorithms. In this technique, each category is represented as a binary vector where all values are zero except for one value, which is set to one to represent the corresponding category.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9m9GdNmMom3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Explain Bag of Words?**\n",
        "\n",
        "**Ans:** It is a technique used to represent text data in a numerical format. In this technique, the text is first tokenized into words, and then a matrix is created that represents the frequency of occurrence of each word in the text. Each row in the matrix represents a document, and each column represents a word.\n",
        "\n"
      ],
      "metadata": {
        "id": "DFMvbMpao3nx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Explain Bag of N-Grams?**\n",
        "\n",
        "**Ans:**It is an extension of the Bag of Words technique where instead of considering individual words, we consider a group of contiguous words, known as N-grams. In this technique, the text is first tokenized into N-grams, and then a matrix is created that represents the frequency of occurrence of each N-gram in the text.\n",
        "\n"
      ],
      "metadata": {
        "id": "mrsj5qkJo9pK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Explain TF-IDF?**\n",
        "\n",
        "**Ans:** It is a technique used to represent text data in a numerical format that takes into account the importance of each word in the text. In this technique, each word is assigned a weight based on its frequency in the text (term frequency) and the frequency of its occurrence across all documents (inverse document frequency).\n",
        "\n"
      ],
      "metadata": {
        "id": "A1az8xo9pAAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What is OOV problem?**\n",
        "\n",
        "**Ans:** The Out of Vocabulary (OOV) problem occurs when a word that is not present in the vocabulary of a machine learning model is encountered during prediction. This problem can be solved by either adding the new word to the vocabulary or by using techniques such as subword encoding.\n",
        "\n"
      ],
      "metadata": {
        "id": "e1px2aOwpCVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. What are word embeddings?**\n",
        "\n",
        "**Ans:** They are a set of techniques used to represent words in a continuous vector space such that semantically similar words are close to each other in the vector space. Word embeddings are commonly used in natural language processing tasks such as language translation, sentiment analysis, and named entity recognition.\n",
        "\n"
      ],
      "metadata": {
        "id": "mYm1lhIKpEoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Explain Continuous bag of words (CBOW)?**\n",
        "\n",
        "**Ans:** It is a word embedding technique where the goal is to predict the target word given a context of surrounding words. In this technique, a neural network is trained on a corpus of text data, where each word is represented as a vector.\n",
        "\n"
      ],
      "metadata": {
        "id": "H1gWsBmxpGp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Explain SkipGram?**\n",
        "\n",
        "**Ans:** It is a word embedding technique that is the opposite of CBOW. In SkipGram, the goal is to predict the surrounding context words given a target word. The neural network is trained to predict the probability distribution of the context words given the target word.\n",
        "\n"
      ],
      "metadata": {
        "id": "5T4PWymqpImY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Explain Glove Embeddings?**\n",
        "\n",
        "**Ans:**They are word embeddings that are based on the co-occurrence statistics of words in a corpus of text data. In this technique, a matrix is created that represents the co-occurrence frequency of each word pair, and then a factorization method is applied to this matrix to obtain the word embeddings. GloVe embeddings have been shown to outperform other word embedding techniques in several natural language processing tasks."
      ],
      "metadata": {
        "id": "_e0XRnUipKNg"
      }
    }
  ]
}